{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd946056",
   "metadata": {},
   "source": [
    "# Cricket Player Performance Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PSHT15H\\Downloads\\RNN\\CANSpreadsheetUtility-master\\CANSpreadsheetUtility-master\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to C:\\Users\\PSHT15H\\.cache\\kagglehub\\datasets\\akarshsinghh\\cricket-player-performance-prediction\\2.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.42M/2.42M [00:01<00:00, 1.39MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\PSHT15H\\.cache\\kagglehub\\datasets\\akarshsinghh\\cricket-player-performance-prediction\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"akarshsinghh/cricket-player-performance-prediction\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226e01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in C:\\Users\\PSHT15H\\.cache\\kagglehub\\datasets\\akarshsinghh\\cricket-player-performance-prediction\\versions\\2:\n",
      "  ball.csv\n",
      "  bat.csv\n",
      "  match.csv\n",
      "\n",
      "CSV files found: ['ball.csv', 'bat.csv', 'match.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check what files are in the downloaded directory\n",
    "print(f\"Files in {path}:\")\n",
    "for file in os.listdir(path):\n",
    "    print(f\"  {file}\")\n",
    "    \n",
    "# Let's also check if there are any CSV files\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "print(f\"\\nCSV files found: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27bb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match dataframe shape: (6199, 15)\n",
      "\n",
      "Column names: ['Unnamed: 0', 'match_number', 'name', 'start_date', 'matchtype', 'series_id', 'match_detail_id', 'scorecard_id', 'title', 'runs', 'over', 'run_rate', 'match_id', 'opp_team_id', 'team_id']\n",
      "\n",
      "Number of rows: 6199\n",
      "Number of columns: 15\n",
      "   Unnamed: 0  match_number                name                 start_date  \\\n",
      "0        1417           1.0  Bangladesh v India  2004-12-22 18:30:00+00:00   \n",
      "1        1418           1.0  Bangladesh v India  2004-12-22 18:30:00+00:00   \n",
      "2        1419           2.0  Bangladesh v India  2004-12-25 18:30:00+00:00   \n",
      "3        1420           2.0  Bangladesh v India  2004-12-25 18:30:00+00:00   \n",
      "4        1421           3.0  Bangladesh v India  2004-12-26 18:30:00+00:00   \n",
      "\n",
      "  matchtype  series_id  match_detail_id  scorecard_id               title  \\\n",
      "0       odi        182              773          1495       India Innings   \n",
      "1       odi        182              773          1496  Bangladesh Innings   \n",
      "2       odi        182              774          1497  Bangladesh Innings   \n",
      "3       odi        182              774          1498       India Innings   \n",
      "4       odi        182              775          1499       India Innings   \n",
      "\n",
      "   runs  over  run_rate  match_id  opp_team_id  team_id  \n",
      "0   245  50.0      4.90       633            7        2  \n",
      "1   234  50.0      4.68       633            2        7  \n",
      "2   229  50.0      4.58       634            2        7  \n",
      "3   214  47.5      4.47       634            7        2  \n",
      "4   348  50.0      6.96       635            7        2  \n"
     ]
    }
   ],
   "source": [
    "# Load the match.csv file into a dataframe\n",
    "match_df = pd.read_csv(os.path.join(path, 'match.csv'))\n",
    "\n",
    "# Display basic information about the dataframe\n",
    "print(f\"Match dataframe shape: {match_df.shape}\")\n",
    "print(f\"\\nColumn names: {list(match_df.columns)}\")\n",
    "print(f\"\\nNumber of rows: {match_df.shape[0]}\")\n",
    "print(f\"Number of columns: {match_df.shape[1]}\")\n",
    "print(match_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'match_number', 'name', 'start_date', 'matchtype',\n",
      "       'series_id', 'match_detail_id', 'scorecard_id', 'title', 'runs', 'over',\n",
      "       'run_rate', 'match_id', 'opp_team_id', 'team_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(match_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'Unnamed: 0',        # index column\n",
    "    'match detail id',   # pure identifier\n",
    "    'scorecard id'       # pure identifier\n",
    "]\n",
    "\n",
    "match_df = match_df.drop(columns=drop_cols, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e05d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining features:\", match_df.shape[1] - 1)  # minus target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb9118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df['start_date'] = pd.to_datetime(match_df['start_date'], errors='coerce')\n",
    "match_df['match_year'] = match_df['start_date'].dt.year\n",
    "match_df['match_month'] = match_df['start_date'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa3088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['match_number', 'name', 'start_date', 'matchtype', 'series_id', 'match_detail_id', 'scorecard_id', 'title', 'runs', 'over', 'run_rate', 'match_id', 'opp_team_id', 'team_id', 'match_year', 'match_month']\n",
      "Final feature count: 16\n"
     ]
    }
   ],
   "source": [
    "# First, let's check what columns are available\n",
    "print(\"Available columns:\", match_df.columns.tolist())\n",
    "\n",
    "# Since 'winner' column doesn't exist, let's use all columns for features\n",
    "X = match_df.copy()\n",
    "print(\"Final feature count:\", X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3bfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in match_df.select_dtypes(include='object').columns:\n",
    "    match_df[col] = le.fit_transform(match_df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acabe741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a target variable based on runs scored\n",
    "# Group by match_id and determine winner based on highest runs\n",
    "match_results = match_df.groupby('match_id')['runs'].transform('max')\n",
    "match_df['winner'] = (match_df['runs'] == match_results).astype(int)\n",
    "\n",
    "X = match_df.drop('winner', axis=1)\n",
    "y = match_df['winner']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad20bab",
   "metadata": {},
   "source": [
    "# Train–Test Split + Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Select only numeric columns for scaling\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "non_numeric_columns = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_columns])\n",
    "\n",
    "# Convert scaled arrays back to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=numeric_columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=numeric_columns, index=X_test.index)\n",
    "\n",
    "# Combine scaled numeric columns with non-numeric columns\n",
    "X_train = pd.concat([X_train_scaled_df, X_train[non_numeric_columns]], axis=1)\n",
    "X_test = pd.concat([X_test_scaled_df, X_test[non_numeric_columns]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472837c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_prob),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred),\n",
    "        \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: ['start_date']\n",
      "\n",
      "Data types in X_train:\n",
      "match_number                   float64\n",
      "name                           float64\n",
      "matchtype                      float64\n",
      "series_id                      float64\n",
      "match_detail_id                float64\n",
      "scorecard_id                   float64\n",
      "title                          float64\n",
      "runs                           float64\n",
      "over                           float64\n",
      "run_rate                       float64\n",
      "match_id                       float64\n",
      "opp_team_id                    float64\n",
      "team_id                        float64\n",
      "match_year                     float64\n",
      "match_month                    float64\n",
      "start_date         datetime64[ns, UTC]\n",
      "dtype: object\n",
      "\n",
      "Missing values in X_train:\n",
      "match_number       57\n",
      "name                0\n",
      "matchtype           0\n",
      "series_id           0\n",
      "match_detail_id     0\n",
      "scorecard_id        0\n",
      "title               0\n",
      "runs                0\n",
      "over                0\n",
      "run_rate            0\n",
      "match_id            0\n",
      "opp_team_id         0\n",
      "team_id             0\n",
      "match_year          0\n",
      "match_month         0\n",
      "start_date          0\n",
      "dtype: int64\n",
      "\n",
      "Processing column: start_date\n",
      "Data type: datetime64[ns, UTC]\n",
      "Converting datetime column start_date to numeric features\n",
      "\n",
      "Handling missing values...\n",
      "Missing values after initial processing:\n",
      "match_number    57\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "Series([], dtype: int64)\n",
      "Total missing values: 0\n",
      "\n",
      "Processed X_train shape: (4959, 18)\n",
      "Processed X_test shape: (1240, 18)\n",
      "Final data types:\n",
      "match_number        float64\n",
      "name                float64\n",
      "matchtype           float64\n",
      "series_id           float64\n",
      "match_detail_id     float64\n",
      "scorecard_id        float64\n",
      "title               float64\n",
      "runs                float64\n",
      "over                float64\n",
      "run_rate            float64\n",
      "match_id            float64\n",
      "opp_team_id         float64\n",
      "team_id             float64\n",
      "match_year          float64\n",
      "match_month         float64\n",
      "start_date_year     float64\n",
      "start_date_month    float64\n",
      "start_date_day      float64\n",
      "dtype: object\n",
      "\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PSHT15H\\Downloads\\RNN\\CANSpreadsheetUtility-master\\CANSpreadsheetUtility-master\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression trained successfully!\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree trained successfully!\n",
      "\n",
      "Training KNN...\n",
      "KNN trained successfully!\n",
      "\n",
      "Training Naive Bayes...\n",
      "Naive Bayes trained successfully!\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest trained successfully!\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost trained successfully!\n",
      "\n",
      "Training completed! 6 models trained successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PSHT15H\\Downloads\\RNN\\CANSpreadsheetUtility-master\\CANSpreadsheetUtility-master\\.venv\\Lib\\site-packages\\xgboost\\training.py:200: UserWarning: [20:55:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:782: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# First, let's properly handle non-numeric columns and missing values before training\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Debug: Check what non-numeric columns we have\n",
    "print(\"Non-numeric columns:\", non_numeric_columns.tolist())\n",
    "print(\"\\nData types in X_train:\")\n",
    "print(X_train.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "# Create a copy for processing\n",
    "X_train_processed = X_train.copy()\n",
    "X_test_processed = X_test.copy()\n",
    "\n",
    "# Handle non-numeric columns (encode them)\n",
    "label_encoders = {}\n",
    "for col in non_numeric_columns:\n",
    "    if col in X_train_processed.columns:\n",
    "        print(f\"\\nProcessing column: {col}\")\n",
    "        print(f\"Data type: {X_train_processed[col].dtype}\")\n",
    "        \n",
    "        # Convert datetime columns to numeric features\n",
    "        if X_train_processed[col].dtype == 'datetime64[ns]' or pd.api.types.is_datetime64_any_dtype(X_train_processed[col]):\n",
    "            print(f\"Converting datetime column {col} to numeric features\")\n",
    "            # Extract numeric features from datetime\n",
    "            X_train_processed[col + '_year'] = X_train_processed[col].dt.year\n",
    "            X_train_processed[col + '_month'] = X_train_processed[col].dt.month\n",
    "            X_train_processed[col + '_day'] = X_train_processed[col].dt.day\n",
    "            X_test_processed[col + '_year'] = X_test_processed[col].dt.year\n",
    "            X_test_processed[col + '_month'] = X_test_processed[col].dt.month\n",
    "            X_test_processed[col + '_day'] = X_test_processed[col].dt.day\n",
    "            # Drop the original datetime column\n",
    "            X_train_processed = X_train_processed.drop(col, axis=1)\n",
    "            X_test_processed = X_test_processed.drop(col, axis=1)\n",
    "        else:\n",
    "            # Use label encoder for other non-numeric columns\n",
    "            le = LabelEncoder()\n",
    "            # Handle NaN values by filling them with a placeholder\n",
    "            X_train_processed[col] = X_train_processed[col].fillna('Unknown')\n",
    "            X_test_processed[col] = X_test_processed[col].fillna('Unknown')\n",
    "            \n",
    "            # Fit on train and transform both train and test\n",
    "            X_train_processed[col] = le.fit_transform(X_train_processed[col].astype(str))\n",
    "            # For test set, handle any unseen labels\n",
    "            test_labels = X_test_processed[col].astype(str)\n",
    "            test_encoded = []\n",
    "            for label in test_labels:\n",
    "                if label in le.classes_:\n",
    "                    test_encoded.append(le.transform([label])[0])\n",
    "                else:\n",
    "                    # Assign to 'Unknown' category if exists, otherwise use 0\n",
    "                    if 'Unknown' in le.classes_:\n",
    "                        test_encoded.append(le.transform(['Unknown'])[0])\n",
    "                    else:\n",
    "                        test_encoded.append(0)\n",
    "            X_test_processed[col] = test_encoded\n",
    "            label_encoders[col] = le\n",
    "\n",
    "# Handle missing values in numeric columns\n",
    "print(f\"\\nHandling missing values...\")\n",
    "print(f\"Missing values after initial processing:\")\n",
    "missing_before = X_train_processed.isnull().sum()\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Use SimpleImputer to fill missing values with median for numeric columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_processed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train_processed), \n",
    "    columns=X_train_processed.columns,\n",
    "    index=X_train_processed.index\n",
    ")\n",
    "X_test_processed = pd.DataFrame(\n",
    "    imputer.transform(X_test_processed), \n",
    "    columns=X_test_processed.columns,\n",
    "    index=X_test_processed.index\n",
    ")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\nMissing values after imputation:\")\n",
    "missing_after = X_train_processed.isnull().sum()\n",
    "print(missing_after[missing_after > 0])\n",
    "print(f\"Total missing values: {X_train_processed.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nProcessed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed.shape}\")\n",
    "print(f\"Final data types:\")\n",
    "print(X_train_processed.dtypes)\n",
    "\n",
    "# Now train the models with the properly processed data\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        results[name] = evaluate_model(model, X_test_processed, y_test)\n",
    "        print(f\"{name} trained successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {str(e)}\")\n",
    "        \n",
    "print(f\"\\nTraining completed! {len(results)} models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a04e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Results:\n",
      "============================================================\n",
      "                     Accuracy     AUC  Precision  Recall      F1     MCC\n",
      "Logistic Regression    0.6919  0.7597     0.6731  0.6615  0.6672  0.3806\n",
      "Decision Tree          0.6548  0.6542     0.6269  0.6442  0.6354  0.3079\n",
      "KNN                    0.4427  0.4617     0.3978  0.3765  0.3869 -0.1233\n",
      "Naive Bayes            0.6306  0.6937     0.6020  0.6166  0.6092  0.2592\n",
      "Random Forest          0.6944  0.7644     0.6748  0.6667  0.6707  0.3856\n",
      "XGBoost                0.7113  0.7856     0.6915  0.6891  0.6903  0.4199\n",
      "\n",
      "Best Models by Metric:\n",
      "----------------------------------------\n",
      "Accuracy: XGBoost (0.7113)\n",
      "AUC: XGBoost (0.7856)\n",
      "Precision: XGBoost (0.6915)\n",
      "Recall: XGBoost (0.6891)\n",
      "F1: XGBoost (0.6903)\n",
      "MCC: XGBoost (0.4199)\n",
      "\n",
      "Best Overall Model (by F1-Score): XGBoost\n",
      "F1-Score: 0.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PSHT15H\\AppData\\Local\\Temp\\ipykernel_36324\\2848786604.py:58: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Display model performance results\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Model Performance Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Find the best performing model for each metric\n",
    "print(\"\\nBest Models by Metric:\")\n",
    "print(\"-\"*40)\n",
    "for metric in results_df.columns:\n",
    "    best_model = results_df[metric].idxmax()\n",
    "    best_score = results_df[metric].max()\n",
    "    print(f\"{metric}: {best_model} ({best_score:.4f})\")\n",
    "\n",
    "# Overall best model based on F1 score (balanced metric)\n",
    "best_overall = results_df['F1'].idxmax()\n",
    "print(f\"\\nBest Overall Model (by F1-Score): {best_overall}\")\n",
    "print(f\"F1-Score: {results_df.loc[best_overall, 'F1']:.4f}\")\n",
    "\n",
    "# Create a simple visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot accuracy for all models\n",
    "plt.subplot(2, 2, 1)\n",
    "results_df['Accuracy'].plot(kind='bar')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.subplot(2, 2, 2)\n",
    "results_df['F1'].plot(kind='bar')\n",
    "plt.title('Model F1-Score Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('F1-Score')\n",
    "\n",
    "# Plot AUC\n",
    "plt.subplot(2, 2, 3)\n",
    "results_df['AUC'].plot(kind='bar')\n",
    "plt.title('Model AUC Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('AUC')\n",
    "\n",
    "# Plot MCC\n",
    "plt.subplot(2, 2, 4)\n",
    "results_df['MCC'].plot(kind='bar')\n",
    "plt.title('Model MCC Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('MCC')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix matplotlib display issue for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Re-create the visualization with proper backend configuration\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot accuracy for all models\n",
    "plt.subplot(2, 2, 1)\n",
    "results_df['Accuracy'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.subplot(2, 2, 2)\n",
    "results_df['F1'].plot(kind='bar', color='lightgreen')\n",
    "plt.title('Model F1-Score Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot AUC\n",
    "plt.subplot(2, 2, 3)\n",
    "results_df['AUC'].plot(kind='bar', color='lightcoral')\n",
    "plt.title('Model AUC Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('AUC')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot MCC\n",
    "plt.subplot(2, 2, 4)\n",
    "results_df['MCC'].plot(kind='bar', color='lightsalmon')\n",
    "plt.title('Model MCC Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('MCC')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'model_comparison.png'\")\n",
    "print(\"\\nPlots should now display properly in the notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128950c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
